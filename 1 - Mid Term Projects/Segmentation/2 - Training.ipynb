{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 256\n",
    "width = 256\n",
    "\n",
    "batch_size = 16\n",
    "lr = 2e-4\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset\"\n",
    "\n",
    "files_dir = \"files\"\n",
    "model_file = os.path.join(files_dir, \"unet.h5\")\n",
    "log_file = os.path.join(files_dir, \"log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention_module(x, ratio=8):\n",
    "    channel = x.shape[-1]\n",
    "    \n",
    "    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=False)\n",
    "    l2 = Dense(channel, use_bias=False)\n",
    "    \n",
    "    x1 = GlobalAveragePooling2D()(x)\n",
    "    x1 = l1(x1)\n",
    "    x1 = l2(x1)\n",
    "    \n",
    "    x2 = GlobalMaxPooling2D()(x)\n",
    "    x2 = l1(x2)\n",
    "    x2 = l2(x2)\n",
    "    \n",
    "    feats = x1 + x2\n",
    "    feats = Activation(\"sigmoid\")(feats)\n",
    "    \n",
    "    feats = Multiply()([x, feats])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention_module(x):\n",
    "    x1 = tf.reduce_mean(x, axis=-1)\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\n",
    "    \n",
    "    x2 = tf.reduce_max(x, axis=-1)\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\n",
    "    \n",
    "    feats = Concatenate()([x1, x2])\n",
    "    feats = Conv2D(1, kernel_size=7, padding=\"same\", activation=\"sigmoid\")(feats)\n",
    "    \n",
    "    feats = Multiply()([x, feats])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam(x):\n",
    "    x = channel_attention_module(x)\n",
    "    x = spatial_attention_module(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = cbam(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, skip, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    \"\"\" ResNet50 Encoder \"\"\"\n",
    "    resnet50 = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "    \n",
    "    s1 = resnet50.get_layer(\"input_1\").output\n",
    "    s2 = resnet50.get_layer(\"conv1_relu\").output\n",
    "    s3 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "    s4 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "    \n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"UNET\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"train\", \"images\", \"*\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"train\", \"masks\", \"*\")))\n",
    "    \n",
    "    valid_x = sorted(glob(os.path.join(path, \"valid\", \"images\", \"*\")))\n",
    "    valid_y = sorted(glob(os.path.join(path, \"valid\", \"masks\", \"*\")))\n",
    "    \n",
    "    return (train_x, train_y), (valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (width, height))\n",
    "    x = x/255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (width, height))\n",
    "    x = x/255.0\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "    \n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
    "    x.set_shape([height, width, 3])\n",
    "    y.set_shape([height, width, 1])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 536 - 536\n",
      "Valid: 67 - 67\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)\n",
    "print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (height, width, 3)\n",
    "model = build_unet(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNET\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 32, 32, 512)  2097664     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 1024) 0           conv2d_transpose[0][0]           \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 512)  4719104     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 512)  2048        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 512)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 512)  2359808     activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 512)  2048        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 512)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d (GlobalMax (None, 512)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           32768       global_average_pooling2d[0][0]   \n",
      "                                                                 global_max_pooling2d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          32768       dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 512)          0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 32, 32, 512)  0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 32, 32)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max (TFOpLambda) (None, 32, 32)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 32, 32, 1)    0           tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 32, 32, 1)    0           tf.math.reduce_max[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 2)    0           tf.expand_dims[0][0]             \n",
      "                                                                 tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 1)    99          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 32, 32, 512)  0           multiply[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 256)  524544      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 512)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 256)  1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 256)  590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 256)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           8192        global_average_pooling2d_1[0][0] \n",
      "                                                                 global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          8192        dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 64, 64, 256)  0           activation_4[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_1 (TFOpLamb (None, 64, 64)       0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_1 (TFOpLambd (None, 64, 64)       0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 64, 64, 1)    0           tf.math.reduce_mean_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 64, 64, 1)    0           tf.math.reduce_max_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 2)    0           tf.expand_dims_2[0][0]           \n",
      "                                                                 tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 1)    99          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 64, 64, 256)  0           multiply_2[0][0]                 \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 128 131200      multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 128, 192 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 128 221312      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 128 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 128 147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 128 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2048        global_average_pooling2d_2[0][0] \n",
      "                                                                 global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          2048        dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 128)          0           dense_5[0][0]                    \n",
      "                                                                 dense_5[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128)          0           tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 128, 128, 128 0           activation_7[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_2 (TFOpLamb (None, 128, 128)     0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_2 (TFOpLambd (None, 128, 128)     0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_4 (TFOpLambda)   (None, 128, 128, 1)  0           tf.math.reduce_mean_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_5 (TFOpLambda)   (None, 128, 128, 1)  0           tf.math.reduce_max_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 2)  0           tf.expand_dims_4[0][0]           \n",
      "                                                                 tf.expand_dims_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 1)  99          concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 128, 128, 128 0           multiply_4[0][0]                 \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 64) 32832       multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256, 256, 67) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 256, 64) 38656       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 256, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256, 256, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 256, 64) 36928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256, 256, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256, 256, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 64)           0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 64)           0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            512         global_average_pooling2d_3[0][0] \n",
      "                                                                 global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           512         dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64)           0           dense_7[0][0]                    \n",
      "                                                                 dense_7[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64)           0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 256, 256, 64) 0           activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_3 (TFOpLamb (None, 256, 256)     0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_3 (TFOpLambd (None, 256, 256)     0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_6 (TFOpLambda)   (None, 256, 256, 1)  0           tf.math.reduce_mean_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_7 (TFOpLambda)   (None, 256, 256, 1)  0           tf.math.reduce_max_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 2)  0           tf.expand_dims_6[0][0]           \n",
      "                                                                 tf.expand_dims_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 1)  99          concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 256, 256, 64) 0           multiply_6[0][0]                 \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 1)  65          multiply_7[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 20,763,981\n",
      "Trainable params: 20,729,549\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    dice = (2. * intersection + 1e-15) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-15)\n",
    "    return 1.0 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "model.compile(loss=dice_loss, optimizer=opt, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10),\n",
    "        CSVLogger(log_file),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 27s 559ms/step - loss: 0.6389 - acc: 0.7760 - val_loss: 0.7968 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79681, saving model to files/unet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/tf/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.5011 - acc: 0.8384 - val_loss: 0.7978 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.79681\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.3853 - acc: 0.8563 - val_loss: 0.7978 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.79681\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.3091 - acc: 0.8627 - val_loss: 0.7977 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.79681\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.2442 - acc: 0.8663 - val_loss: 0.7985 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.79681\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.2030 - acc: 0.8685 - val_loss: 0.7996 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.79681\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.1761 - acc: 0.8698 - val_loss: 0.8018 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.79681\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.1596 - acc: 0.8704 - val_loss: 0.8269 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.79681\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.1460 - acc: 0.8711 - val_loss: 0.8263 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.79681\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.1371 - acc: 0.8712 - val_loss: 0.8703 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79681\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.1277 - acc: 0.8716 - val_loss: 0.8486 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.79681\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.1198 - acc: 0.8724 - val_loss: 0.8713 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.79681\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.1146 - acc: 0.8732 - val_loss: 0.8886 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.79681\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.1126 - acc: 0.8734 - val_loss: 0.8989 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.79681\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.1111 - acc: 0.8736 - val_loss: 0.9056 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.79681\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.1097 - acc: 0.8738 - val_loss: 0.9074 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.79681\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.1084 - acc: 0.8739 - val_loss: 0.9019 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.79681\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.1070 - acc: 0.8741 - val_loss: 0.8841 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.79681\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.1057 - acc: 0.8742 - val_loss: 0.8530 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.79681\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.1044 - acc: 0.8744 - val_loss: 0.8076 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.79681\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.1030 - acc: 0.8746 - val_loss: 0.7458 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.79681 to 0.74583, saving model to files/unet.h5\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.1017 - acc: 0.8747 - val_loss: 0.6762 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.74583 to 0.67618, saving model to files/unet.h5\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.1005 - acc: 0.8749 - val_loss: 0.6001 - val_acc: 0.8337\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.67618 to 0.60009, saving model to files/unet.h5\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.0992 - acc: 0.8750 - val_loss: 0.5278 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.60009 to 0.52782, saving model to files/unet.h5\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0979 - acc: 0.8752 - val_loss: 0.4660 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.52782 to 0.46604, saving model to files/unet.h5\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0966 - acc: 0.8753 - val_loss: 0.3980 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.46604 to 0.39797, saving model to files/unet.h5\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0954 - acc: 0.8755 - val_loss: 0.3429 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.39797 to 0.34292, saving model to files/unet.h5\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0943 - acc: 0.8756 - val_loss: 0.2715 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34292 to 0.27153, saving model to files/unet.h5\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0932 - acc: 0.8757 - val_loss: 0.2264 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27153 to 0.22642, saving model to files/unet.h5\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0922 - acc: 0.8758 - val_loss: 0.1854 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.22642 to 0.18538, saving model to files/unet.h5\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0911 - acc: 0.8759 - val_loss: 0.1519 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.18538 to 0.15188, saving model to files/unet.h5\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0904 - acc: 0.8760 - val_loss: 0.1344 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.15188 to 0.13436, saving model to files/unet.h5\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0896 - acc: 0.8760 - val_loss: 0.1261 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.13436 to 0.12606, saving model to files/unet.h5\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0882 - acc: 0.8761 - val_loss: 0.1255 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.12606 to 0.12551, saving model to files/unet.h5\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0871 - acc: 0.8761 - val_loss: 0.1188 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.12551 to 0.11877, saving model to files/unet.h5\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 14s 425ms/step - loss: 0.0856 - acc: 0.8763 - val_loss: 0.1189 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11877\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0841 - acc: 0.8764 - val_loss: 0.1212 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11877\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0831 - acc: 0.8765 - val_loss: 0.1177 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.11877 to 0.11772, saving model to files/unet.h5\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0823 - acc: 0.8765 - val_loss: 0.1175 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11772 to 0.11747, saving model to files/unet.h5\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0815 - acc: 0.8766 - val_loss: 0.1213 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11747\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0805 - acc: 0.8767 - val_loss: 0.1187 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11747\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0796 - acc: 0.8768 - val_loss: 0.1216 - val_acc: 0.8839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11747\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0790 - acc: 0.8768 - val_loss: 0.1219 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11747\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0783 - acc: 0.8768 - val_loss: 0.1173 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.11747 to 0.11733, saving model to files/unet.h5\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0775 - acc: 0.8769 - val_loss: 0.1176 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11733\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 15s 429ms/step - loss: 0.0764 - acc: 0.8770 - val_loss: 0.1162 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.11733 to 0.11616, saving model to files/unet.h5\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0759 - acc: 0.8770 - val_loss: 0.1162 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11616\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0758 - acc: 0.8770 - val_loss: 0.1176 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11616\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 15s 429ms/step - loss: 0.0759 - acc: 0.8769 - val_loss: 0.1159 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11616 to 0.11595, saving model to files/unet.h5\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0764 - acc: 0.8768 - val_loss: 0.1142 - val_acc: 0.8835\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11595 to 0.11423, saving model to files/unet.h5\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0767 - acc: 0.8766 - val_loss: 0.1115 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.11423 to 0.11147, saving model to files/unet.h5\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0761 - acc: 0.8766 - val_loss: 0.1117 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11147\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 15s 427ms/step - loss: 0.0754 - acc: 0.8767 - val_loss: 0.1199 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11147\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0760 - acc: 0.8766 - val_loss: 0.1227 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11147\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 15s 427ms/step - loss: 0.0750 - acc: 0.8767 - val_loss: 0.1151 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11147\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0721 - acc: 0.8771 - val_loss: 0.1107 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.11147 to 0.11074, saving model to files/unet.h5\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0703 - acc: 0.8774 - val_loss: 0.1101 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.11074 to 0.11007, saving model to files/unet.h5\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0688 - acc: 0.8776 - val_loss: 0.1081 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.11007 to 0.10805, saving model to files/unet.h5\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0678 - acc: 0.8777 - val_loss: 0.1094 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.10805\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0671 - acc: 0.8777 - val_loss: 0.1114 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.10805\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0665 - acc: 0.8778 - val_loss: 0.1121 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10805\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0658 - acc: 0.8778 - val_loss: 0.1124 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.10805\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 15s 427ms/step - loss: 0.0654 - acc: 0.8778 - val_loss: 0.1142 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10805\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0653 - acc: 0.8778 - val_loss: 0.1133 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10805\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0644 - acc: 0.8778 - val_loss: 0.1100 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10805\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0641 - acc: 0.8778 - val_loss: 0.1099 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.10805\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0634 - acc: 0.8779 - val_loss: 0.1092 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10805\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 14s 426ms/step - loss: 0.0629 - acc: 0.8779 - val_loss: 0.1094 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10805\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0644 - acc: 0.8776 - val_loss: 0.1072 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.10805 to 0.10718, saving model to files/unet.h5\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 15s 438ms/step - loss: 0.0610 - acc: 0.8782 - val_loss: 0.1073 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.10718\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 15s 436ms/step - loss: 0.0607 - acc: 0.8782 - val_loss: 0.1071 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.10718 to 0.10708, saving model to files/unet.h5\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0603 - acc: 0.8783 - val_loss: 0.1070 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.10708 to 0.10701, saving model to files/unet.h5\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0601 - acc: 0.8783 - val_loss: 0.1070 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.10701 to 0.10697, saving model to files/unet.h5\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0598 - acc: 0.8784 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.10697 to 0.10695, saving model to files/unet.h5\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0597 - acc: 0.8784 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.10695 to 0.10694, saving model to files/unet.h5\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0595 - acc: 0.8784 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10694 to 0.10694, saving model to files/unet.h5\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0594 - acc: 0.8784 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.10694 to 0.10693, saving model to files/unet.h5\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0592 - acc: 0.8784 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10693\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.0591 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10693 to 0.10693, saving model to files/unet.h5\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.0590 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.10693 to 0.10692, saving model to files/unet.h5\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.0589 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10692 to 0.10691, saving model to files/unet.h5\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.0588 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.10691 to 0.10689, saving model to files/unet.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0587 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10689 to 0.10689, saving model to files/unet.h5\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0586 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.10689 to 0.10687, saving model to files/unet.h5\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 15s 433ms/step - loss: 0.0585 - acc: 0.8785 - val_loss: 0.1069 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10687 to 0.10686, saving model to files/unet.h5\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 15s 432ms/step - loss: 0.0584 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10686 to 0.10685, saving model to files/unet.h5\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0583 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10685 to 0.10684, saving model to files/unet.h5\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0582 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.10684 to 0.10682, saving model to files/unet.h5\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.0581 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10682 to 0.10680, saving model to files/unet.h5\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 15s 434ms/step - loss: 0.0580 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.10680 to 0.10678, saving model to files/unet.h5\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 15s 430ms/step - loss: 0.0579 - acc: 0.8786 - val_loss: 0.1068 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10678 to 0.10676, saving model to files/unet.h5\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0579 - acc: 0.8786 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.10676 to 0.10675, saving model to files/unet.h5\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 15s 427ms/step - loss: 0.0578 - acc: 0.8786 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10675 to 0.10673, saving model to files/unet.h5\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0577 - acc: 0.8786 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.10673 to 0.10672, saving model to files/unet.h5\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0576 - acc: 0.8786 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.10672 to 0.10670, saving model to files/unet.h5\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0575 - acc: 0.8786 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.10670 to 0.10669, saving model to files/unet.h5\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0574 - acc: 0.8787 - val_loss: 0.1067 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.10669 to 0.10667, saving model to files/unet.h5\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0573 - acc: 0.8787 - val_loss: 0.1066 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.10667 to 0.10665, saving model to files/unet.h5\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 15s 428ms/step - loss: 0.0573 - acc: 0.8787 - val_loss: 0.1066 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.10665 to 0.10663, saving model to files/unet.h5\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 14s 427ms/step - loss: 0.0572 - acc: 0.8787 - val_loss: 0.1066 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.10663 to 0.10661, saving model to files/unet.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f247c640280>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=valid_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
